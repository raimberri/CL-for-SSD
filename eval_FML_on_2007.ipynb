{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from datasets import PascalVOCDataset\n",
    "from tqdm import tqdm\n",
    "from pprint import PrettyPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good formatting when printing the APs for each class and mAP\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Parameters\n",
    "data_folder = './JSON'\n",
    "keep_difficult = True  # difficult ground truth objects must always be considered in mAP calculation, because these objects DO exist!\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = './Checkpoint/checkpoint_ssd300_wo_exemplar.pth.tar'\n",
    "\n",
    "# Load model checkpoint that is to be evaluated\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                year='2007',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                          collate_fn=test_dataset.collate_fn, num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate.\n",
    "\n",
    "    :param test_loader: DataLoader for test data\n",
    "    :param model: model\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure it's in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store detected and true boxes, labels, scores\n",
    "    det_boxes = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels = list()\n",
    "    true_difficulties = list()  # it is necessary to know which objects are 'difficult', see 'calculate_mAP' in utils.py\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
    "            images = images.to(device)  # (N, 3, 300, 300)\n",
    "\n",
    "            # Forward prop.\n",
    "            predicted_locs, predicted_scores, _ = model(images)\n",
    "\n",
    "            # Detect objects in SSD output\n",
    "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
    "                                                                                       min_score=0.01, max_overlap=0.45,\n",
    "                                                                                       top_k=200)\n",
    "            # Evaluation MUST be at min_score=0.01, max_overlap=0.45, top_k=200 for fair comparision with the paper's results and other repos\n",
    "\n",
    "            # Store this batch's results for mAP calculation\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            difficulties = [d.to(device) for d in difficulties]\n",
    "\n",
    "            det_boxes.extend(det_boxes_batch)\n",
    "            det_labels.extend(det_labels_batch)\n",
    "            det_scores.extend(det_scores_batch)\n",
    "            true_boxes.extend(boxes)\n",
    "            true_labels.extend(labels)\n",
    "            true_difficulties.extend(difficulties)\n",
    "\n",
    "        # Calculate mAP\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
    "\n",
    "    # Print AP for each class\n",
    "    pp.pprint(APs)\n",
    "\n",
    "    print('\\nMean Average Precision (mAP): %.3f' % mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 78/78 [02:31<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aeroplane': 0.7484603524208069,\n",
      " 'bicycle': 0.7937467694282532,\n",
      " 'bird': 0.6827834248542786,\n",
      " 'boat': 0.6262481808662415,\n",
      " 'bottle': 0.32710081338882446,\n",
      " 'bus': 0.8271308541297913,\n",
      " 'car': 0.8098090291023254,\n",
      " 'cat': 0.8594765663146973,\n",
      " 'chair': 0.4918886721134186,\n",
      " 'cow': 0.7675068974494934,\n",
      " 'diningtable': 0.7208981513977051,\n",
      " 'dog': 0.8258644938468933,\n",
      " 'horse': 0.830863893032074,\n",
      " 'motorbike': 0.8102075457572937,\n",
      " 'person': 0.7357665300369263,\n",
      " 'pottedplant': 0.39048343896865845,\n",
      " 'sheep': 0.750926673412323,\n",
      " 'sofa': 0.7297662496566772,\n",
      " 'train': 0.8013293743133545,\n",
      " 'tvmonitor': 0.7291537523269653}\n",
      "\n",
      "Mean Average Precision (mAP): 0.713\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
